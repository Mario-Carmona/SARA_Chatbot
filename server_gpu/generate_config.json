{
    "conversational": {
        "do_sample": true,
        "temperature": 0.0,
        "top-p": 1.0,
        "max_new_tokens": 64
    }
}