{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mario-Carmona/ProyectoFinal-VC/blob/main/ProyectoFinal.ipynb)"
      ],
      "metadata": {
        "id": "mlk22Pcv8B7R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLzX_EPqAnEW"
      },
      "source": [
        "### Fine-tuning 6-Billion GPT-J in colab with LoRA and 8-bit compression\n",
        "\n",
        "This notebook is a proof of concept for fine-tuning [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) with limited memory. A detailed explanation of how it works can be found in [this model card](https://huggingface.co/hivemind/gpt-j-6B-8bit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op0GXmC8CCyR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.14.1\n",
        "!pip install bitsandbytes-cuda111==0.26.0\n",
        "!pip install datasets==1.16.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0dy1ZFwClcq"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GODiktIBFt4w"
      },
      "source": [
        "### Converting the model to 8 bits.\n",
        "\n",
        "We convert EleutherAI's GPT-J-6B model to 8 bits using facebook's [bitsandbytes](https://github.com/facebookresearch/bitsandbytes) library. This reduces the model's size from 20Gb down to just 6Gb.\n",
        "\n",
        "Note that we don't convert linear layer biases to 8 bit as they take up less that 1% of the model's weight anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8Y75B6WDIN-"
      },
      "outputs": [],
      "source": [
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        " \n",
        "    def forward(self, input):\n",
        "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        " \n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        " \n",
        " \n",
        "class DequantizeAndLinear(torch.autograd.Function): \n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        " \n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        " \n",
        " \n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        " \n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output \n",
        " \n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        " \n",
        " \n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        " \n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        " \n",
        " \n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr( \n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOSZ-S1cDRq1"
      },
      "outputs": [],
      "source": [
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "        \n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pthEhmDBSyEm"
      },
      "outputs": [],
      "source": [
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuW4H6HTS82r"
      },
      "outputs": [],
      "source": [
        "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRF2QqvzENCr"
      },
      "source": [
        "### Text generation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJd45kFaZ1Ba"
      },
      "outputs": [],
      "source": [
        "prompt = tokenizer(\"A cat sat on a mat\", return_tensors='pt')\n",
        "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "out = gpt.generate(**prompt, min_length=128, max_length=128, do_sample=True)\n",
        "tokenizer.decode(out[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfdLQHOuEU7h"
      },
      "source": [
        "### LoRA fine-tuning example\n",
        "Here we demonstrate how to fine-tune the proposed model using low-rank adapters [(Hu et al, 2021)](https://arxiv.org/abs/2106.09685) and [8-bit Adam](https://arxiv.org/abs/2110.02861). We also use [dataset streaming API](https://huggingface.co/docs/datasets/dataset_streaming.html) to avoid downloading the large dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5ctu4Q5aq-g"
      },
      "outputs": [],
      "source": [
        "def add_adapters(model, adapter_dim=16):\n",
        "    assert adapter_dim > 0\n",
        "\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, FrozenBNBLinear):\n",
        "            module.adapter = nn.Sequential(\n",
        "                nn.Linear(module.in_features, adapter_dim, bias=False),\n",
        "                nn.Linear(adapter_dim, module.out_features, bias=False),\n",
        "            )\n",
        "            nn.init.zeros_(module.adapter[1].weight)\n",
        "        elif isinstance(module, FrozenBNBEmbedding):\n",
        "            module.adapter = nn.Sequential(\n",
        "                nn.Embedding(module.num_embeddings, adapter_dim),\n",
        "                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n",
        "            )\n",
        "            nn.init.zeros_(module.adapter[1].weight)\n",
        "\n",
        "add_adapters(gpt)\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIlHG9Wk0WaJ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from bitsandbytes.optim import Adam8bit\n",
        "\n",
        "gpt.gradient_checkpointing_enable()\n",
        "\n",
        "codeparrot = load_dataset(\"transformersbook/codeparrot-train\", streaming=True)\n",
        "optimizer = Adam8bit(gpt.parameters(), lr=1e-5)\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "    for row in tqdm(codeparrot[\"train\"]):\n",
        "        if len(row[\"content\"]) <= 1:\n",
        "            continue\n",
        "\n",
        "        batch = tokenizer(row[\"content\"], truncation=True, max_length=128, return_tensors='pt')\n",
        "        batch = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "        out = gpt.forward(**batch,)\n",
        "\n",
        "        loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n",
        "                               reduction='mean')\n",
        "        print(loss)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This training loop is just a proof of concept - to show that even in the heaviest case, it still fits on a gpu.\n",
        "Depending on your finetuning task, you'll need to remove some parts.\n",
        "Below we explain how to modify the code to achieve the setup from the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf)\n",
        "\n",
        "If you wanna fine-tune a-la LoRA , please use the parameters from Table 11,12 and 15 as a starter:\n",
        "\n",
        "(1) Train only the adapter matrices from attention layers\n",
        "\n",
        "In the above example, we train all kinds of adapters, and also layernorm scales and biases. This is only useful for fine-tuning over reasonably large datasets over long time.\n",
        "For quick setups you should tag everything except **the attention adapters** as `requires_grad=False` -- or just don't feed them into Adam:\n",
        "\n",
        "```\n",
        "\n",
        "params_for_optimizer = [\n",
        "    param for name, param in model.named_parameters()\n",
        "    if \"attn\" in name and \"adapter\" in name\n",
        "]\n",
        "print(\"Trainiable params:\", len(params_for_optimizer))\n",
        "\n",
        "# and after you verified it:\n",
        "for name, param in model.named_parameters():\n",
        "    if param not in params_for_optimizer:\n",
        "        print(f\"Setting {name} requires_grad=False\")\n",
        "        param.requires_grad = False\n",
        "```\n",
        "\n",
        "An even better way is to only create adapters that you need by modifying the `add_adapters` function above:\n",
        "```\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, (FrozenBNBLinear, FrozenBNBEmbedding)):\n",
        "        if \"attn\" in name:\n",
        "            print(\"Adding adapter to\", name)\n",
        "\n",
        "            todo_initialize_adapters_like_in_notebook()\n",
        "        else:\n",
        "            print(\"Not adding adapter to\", name)\n",
        "```\n",
        "As a side-effect, that would actually somewhat reduce the memory usage and may let you fit a longer sequence (e.g. 256)\n",
        "\n",
        "\n",
        "(2) initialize the second adapter matrix with zeros\n",
        "```\n",
        "for name, module in model.named_modules():\n",
        "    if hasattr(module, \"adapter\"):\n",
        "        print(\"Initializing\", name)\n",
        "        nn.init.zeros_(module.adapter[1].weight)\n",
        "        # optional: scale adapter[0].weight by (LoRA_alpha / r)\n",
        "```\n",
        "\n",
        "(3) use warmup and weight decay in Adam:\n",
        "```\n",
        "optimizer = Adam8Bit(..., weight_decay=0.01)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps_from_paper(), expected_total_number_of_steps\n",
        ")\n",
        "\n",
        "actually_use_scheduler_in_training_loop()\n",
        "```\n",
        "\n",
        "Finally, we recommend modifying training loop to track the training metrics, saving the best checkpoint, etc."
      ],
      "metadata": {
        "id": "lx1T8Qvga-fN"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "finetune-gpt-j-6B-8bit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}